$OpenBSD: patch-ECtools_zarafa-search_ECAnalyzers_cpp,v 1.2 2014/04/07 13:46:06 robert Exp $

Fix buil with clucene-core >= 2.x

--- ECtools/zarafa-search/ECAnalyzers.cpp.orig	Mon Mar 31 13:45:37 2014
+++ ECtools/zarafa-search/ECAnalyzers.cpp	Sun Apr  6 19:53:26 2014
@@ -79,24 +79,24 @@ EmailFilter::~EmailFilter() {
  * @param token Output token
  * @return false if no more token was available
  */
-bool EmailFilter::next(lucene::analysis::Token *token) {
+lucene::analysis::Token *EmailFilter::next(lucene::analysis::Token *token) {
 	// See if we had any stored tokens
 	if(part < parts.size()) {
 		token->set(parts[part].c_str(), 0, 0, _T("<EMAIL>"));
 		token->setPositionIncrement(0);
 		part++;
-		return true;
+		return token;
 	} else {
 		// No more stored token, get a new one
 		if(!input->next(token))
-			return false;
+			return NULL;
 
 		// Split EMAIL tokens into the various parts
 		if(wcscmp(token->type(), L"<EMAIL>") == 0) {
 			// Split into user, domain, com
-			parts = tokenize((std::wstring)token->_termText, (std::wstring)L".@");
+			parts = tokenize((std::wstring)token->termBuffer(), (std::wstring)L".@");
 			// Split into user, domain.com
-			std::vector<std::wstring> moreparts = tokenize((std::wstring)token->_termText, (std::wstring)L"@");
+			std::vector<std::wstring> moreparts = tokenize((std::wstring)token->termBuffer(), (std::wstring)L"@");
 			parts.insert(parts.end(), moreparts.begin(), moreparts.end());
 			
 			// Only add parts once (unique parts)
@@ -111,7 +111,7 @@ bool EmailFilter::next(lucene::analysis::Token *token)
 			std::vector<std::wstring> hostparts;
 			
 			// Convert into some-strange.domain.com domain.com com
-			hostparts = tokenize((std::wstring)token->_termText, (std::wstring)L".");
+			hostparts = tokenize((std::wstring)token->termBuffer(), (std::wstring)L".");
 			
 			parts.clear();
 			while(hostparts.size() > 1 && hostparts.size() < 10) { // 10 as a defensive measure against the following blowing up
@@ -122,7 +122,7 @@ bool EmailFilter::next(lucene::analysis::Token *token)
 			part = 0;
 		}
 		
-		return true;
+		return token;
 	}
 }
 
@@ -141,7 +141,7 @@ ECAnalyzer::~ECAnalyzer()
  * @param reader Reader to read the bytestream to tokenize
  * @return A TokenStream outputting the tokens to be indexed
  */
-lucene::analysis::TokenStream* ECAnalyzer::tokenStream(const TCHAR* fieldName, lucene::util::Reader* reader) 
+lucene::analysis::TokenStream *ECAnalyzer::tokenStream(const TCHAR *fieldName, CL_NS(util)::BufferedReader *reader)
 {
 	lucene::analysis::TokenStream* ret = _CLNEW lucene::analysis::standard::StandardTokenizer(reader);
 	ret = _CLNEW lucene::analysis::standard::StandardFilter(ret,true);
